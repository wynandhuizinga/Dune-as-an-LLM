## Dune as an LLM
This repository is merely a demonstration of what would happen if you transform Frank Herbert's Dune saga into an LLM. The main research question being: How big does a large corpus of text need to be in order to train an LLM to come up with sensible sentences in response. Although the result is mostly producing gibberish, it can be observed that the model is responsive and producing text which can be 'interpretted', certainly the response doesn't make too much sense. Have a look:

![Demo](https://github.com/wynandhuizinga/Dune-as-an-LLM/blob/main/Demo.JPG)

This model is obviously not fine-tuned. It also follows a chat-completion approach as opposed to modern gpt's. 


#### Acknowledgement
Credits to Elliot Arledge and freeCodeCamps.org for arranging the coarse on which this repository was inspired.
